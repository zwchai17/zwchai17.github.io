@article{zeng2025glm,
  title={Glm-4.5: Agentic, reasoning, and coding (arc) foundation models},
  author={{Contributor to the GLM Team}},
  journal={arXiv preprint arXiv:2508.06471},
  year={2025},
  selected={true},
  abbr={Technical Report},
  google_scholar_id={LkGwnXOMwfcC},
  abstract={We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.},
  url={https://arxiv.org/abs/2508.06471},
  html={https://arxiv.org/abs/2508.06471},
  pdf={https://arxiv.org/pdf/2508.06471},
  preview={glm.jpg}
}



@inproceedings{sucypher,
  title={Cypher-RI: Reinforcement Learning for Integrating Schema Selection into Cypher Generation},
  author={Su, Hanchen and Li, Xuyuan and Zhou, Yan and Chai, Ziwei and Wang, Haozheng and Zhang, Chen and Yang, Yang},
  booktitle={Thirty-ninth Annual Conference on Neural Information Processing Systems},
  year={2025},
  abbr={NeurIPS},
  selected={true},
  google_scholar_id={roLk4NBRz8UC},
  abstract={The increasing utilization of graph databases across various fields stems from their capacity to represent intricate interconnections. Nonetheless, exploiting the full capabilities of graph databases continues to be a significant hurdle, largely because of the inherent difficulty in translating natural language into Cypher. Recognizing the critical role of schema selection in database query generation and drawing inspiration from recent progress in reasoning-augmented approaches trained through reinforcement learning to enhance inference capabilities and generalization, we introduce Cypher-RI, a specialized framework for the Text-to-Cypher task. Distinct from conventional approaches, our methodology seamlessly integrates schema selection within the Cypher generation pipeline, conceptualizing it as a critical element in the reasoning process. The schema selection mechanism is guided by textual context, with its outcomes recursively shaping subsequent inference processes. Impressively, our 7B-parameter model, trained through this RL paradigm, demonstrates superior performance compared to baselines, exhibiting a 9.41\% accuracy improvement over GPT-4o on CypherBench. These results underscore the effectiveness of our proposed reinforcement learning framework, which integrates schema selection to enhance both the accuracy and reasoning capabilities in Text-to-Cypher tasks.},
  url={https://openreview.net/forum?id=4zhhgKkVzY},
  html={https://openreview.net/forum?id=4zhhgKkVzY},
  pdf={https://openreview.net/pdf?id=4zhhgKkVzY},
  preview={cypher.jpg}
}

@inproceedings{chai2024expert,
  title={An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing},
  author={Chai, Ziwei and Wang, Guoyin and Su, Jing and Zhang, Tianjie and Huang, Xuanwen and Wang, Xuwu and Xu, Jingjing and Yuan, Jianbo and Yang, Hongxia and Wu, Fei and Yang, Yang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024},
  abbr={ACL},
  selected={true},
  google_scholar_id={W7OEmFMy1HYC},
  abstract={We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.},
  url={https://arxiv.org/abs/2403.16854},
  html={https://arxiv.org/html/2403.16854v3},
  pdf={https://arxiv.org/pdf/2403.16854v3},
  preview={acl.jpg}
}


@inproceedings{huang2024gnn,
  title={Can GNN be Good Adapter for LLMs?},
  author = {Huang, Xuanwen and Han, Kaiqiao and Yang, Yang and Bao, Dezheng and Tao, Quanjin and Chai, Ziwei and Zhu, Qi},
  booktitle={Proceedings of the ACM Web Conference},
  year={2024},
  abbr={WWW},
  selected={true},
  google_scholar_id={YsMSGLbcyi4C},
  abstract={Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs. The entire framework is trained using auto-regression on node text (next token prediction). Once trained, GraphAdapter can be seamlessly fine-tuned with task-specific prompts for various downstream tasks. Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5\% in terms of node classification. Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2. The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling.},
  url={https://doi.org/10.1145/3589334.3645627},
  html={https://arxiv.org/html/2402.12984v1},
  pdf={https://arxiv.org/pdf/2402.12984v1},
  preview={gnn.png}
}


@inproceedings{huang2025enhancing,
  title={Enhancing cross-domain link prediction via evolution process modeling},
  author={Huang, Xuanwen and Chow, Wei and Zhu, Yize and Wang, Yang and Chai, Ziwei and Wang, Chunping and Chen, Lei and Yang, Yang},
  booktitle={Proceedings of the ACM on Web Conference},
  pages={2158--2171},
  abbr={WWW},
  year={2025},
  google_scholar_id={roLk4NBRz8UC},
  abstract={The increasing utilization of graph databases across various fields stems from their capacity to represent intricate interconnections. Nonetheless, exploiting the full capabilities of graph databases continues to be a significant hurdle, largely because of the inherent difficulty in translating natural language into Cypher. Recognizing the critical role of schema selection in database query generation and drawing inspiration from recent progress in reasoning-augmented approaches trained through reinforcement learning to enhance inference capabilities and generalization, we introduce Cypher-RI, a specialized framework for the Text-to-Cypher task. Distinct from conventional approaches, our methodology seamlessly integrates schema selection within the Cypher generation pipeline, conceptualizing it as a critical element in the reasoning process. The schema selection mechanism is guided by textual context, with its outcomes recursively shaping subsequent inference processes. Impressively, our 7B-parameter model, trained through this RL paradigm, demonstrates superior performance compared to baselines, exhibiting a 9.41\% accuracy improvement over GPT-4o on CypherBench. These results underscore the effectiveness of our proposed reinforcement learning framework, which integrates schema selection to enhance both the accuracy and reasoning capabilities in Text-to-Cypher tasks.},
  url={https://openreview.net/forum?id=4zhhgKkVzY},
  html={https://openreview.net/forum?id=4zhhgKkVzY},
  pdf={https://openreview.net/pdf?id=4zhhgKkVzY},
}




@inproceedings{hu2024infiagent,
  title={Infiagent-dabench: Evaluating agents on data analysis tasks},
  author={Hu, Xueyu and Zhao, Ziyu and Wei, Shuang and Chai, Ziwei and Wang, Guoyin and Wang, Xuwu and Su, Jing and Xu, Jingjing and Zhu, Ming and Cheng, Yao and Yuan, Jianbo and Kuang, Kun and Yang, Yang and Yang, Hongxia and Wu, Fei},
  booktitle={International Conference on Machine Learning},
  year={2024},
  abbr={ICML},
  selected={true},
  google_scholar_id={Y0pCki6q_DkC},
  abstract={In this paper, we introduce InfiAgent-DABench, the first benchmark specifically designed to evaluate LLM-based agents on data analysis tasks. These tasks require agents to end-to-end solving complex tasks by interacting with an execution environment. This benchmark contains DAEval, a dataset consisting of 257 data analysis questions derived from 52 CSV files, and an agent framework which incorporates LLMs to serve as data analysis agents for both serving and evaluation. Since data analysis questions are often open-ended and hard to evaluate without human supervision, we adopt a format-prompting technique to convert each question into a closed-form format so that they can be automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the current challenges encountered in data analysis tasks. In addition, building on top of our agent framework, we develop a specialized agent, DAAgent, which surpasses GPT-3.5 by 3.9% on DABench. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent .},
  url={https://arxiv.org/abs/2401.05507},
  html={https://arxiv.org/html/2401.05507v3},
  pdf={https://arxiv.org/pdf/2401.05507v3},
  preview={da.jpg}
}


@article{chai2023graphllm,
  title={Graphllm: Boosting graph reasoning ability of large language model},
  author={Chai, Ziwei and Zhang, Tianjie and Wu, Liang and Han, Kaiqiao and Hu, Xiaohai and Huang, Xuanwen and Yang, Yang},
  journal={IEEE Transactions on Big Data},
  year={2025},
  selected={true},
  abbr={TBD},
  google_scholar_id={zYLM7Y9cAGgC},
  abstract={The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This synergy equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of 54.44%, alongside a noteworthy context reduction of 96.45% across various graph reasoning tasks.},
  url={https://arxiv.org/abs/2310.05845},
  html={https://arxiv.org/abs/2310.05845},
  pdf={https://arxiv.org/pdf/2310.05845},
  preview={graphllm.jpg}
}



@inproceedings{chai2023towards,
  title={Towards Learning to Discover Money Laundering Sub-network in Massive Transaction Network},
  author={Chai, Ziwei and Yang, Yang and Dan, Jiawang and Tian, Sheng and Meng, Changhua and Wang, Weiqiang and Sun, Yifei},
  booktitle={Thirty-Seventh AAAI Conference on Artificial Intelligence},
  volume={37},
  year={2023},
  abbr={AAAI},
  selected={true},
  google_scholar_id={9yKSN-GCB0IC},
  abstract={Anti-money laundering (AML) systems play a critical role in safeguarding global economy. As money laundering is considered as one of the top group crimes, there is a crucial need to discover money laundering sub-network behind a particular money laundering transaction for a robust AML system. However, existing rule-based methods for money laundering sub-network discovery is heavily based on domain knowledge and may lag behind the modus operandi of launderers. Therefore, in this work, we first address the money laundering sub-network discovery problem with a neural network based approach, and propose an AML framework AMAP equipped with an adaptive sub-network proposer. In particular, we design an adaptive sub-network proposer guided by a supervised contrastive loss to discriminate money laundering transactions from massive benign transactions. We conduct extensive experiments on real-word datasets in AliPay of Ant Group. The result demonstrates the effectiveness of our AMAP in both money laundering transaction detection and money laundering sub-network discovering. The learned framework which yields money laundering sub-network from massive transaction network leads to a more comprehensive risk coverage and a deeper insight to money laundering strategies.},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/26656},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/26656},
  pdf={http://yangy.org/works/gnn/AAAI23_Laundering.pdf},
  preview={aml.jpg}
}


@inproceedings{chai2022can,
  title={Can abnormality be detected by graph neural networks?},
  author={Chai, Ziwei and You, Siqi and Yang, Yang and Pu, Shiliang and Xu, Jiarong and Cai, Haoyang and Jiang, Weihao},
  booktitle={International Joint Conferences on Artificial Intelligence},
  pages={1945--1951},
  year={2022},
  selected={true},
  abbr={IJCAI},
  google_scholar_id={d1gkVwhDpl0C},
  abstract={Anomaly detection in graphs has attracted considerable interests in both academia and industry due to its wide applications in numerous domains ranging from finance to biology. Meanwhile, graph neural networks (GNNs) is emerging as a powerful tool for modeling graph data. A natural and fundamental question that arises here is: can abnormality be detected by graph neural networks? In this paper, we aim to answer this question, which is nontrivial. As many existing works have explored, graph neural networks can be seen as filters for graph signals, with the favor of low frequency in graphs. In other words, GNN will smooth the signals of adjacent nodes. However, abnormality in a graph intuitively has the characteristic that it tends to be dissimilar to its neighbors, which are mostly normal samples. It thereby conflicts with the general assumption with traditional GNNs. To solve this, we propose a novel Adaptive Multi-frequency Graph Neural Network (AMNet), aiming to capture both low-frequency and high-frequency signals, and adaptively combine signals of different frequencies. Experimental results on real-world datasets demonstrate that our model achieves a significant improvement comparing with several state-of-the-art baseline methods.},
  url={https://www.ijcai.org/proceedings/2022/270},
  html={https://www.ijcai.org/proceedings/2022/270},
  pdf={https://www.ijcai.org/proceedings/2022/0270.pdf},
  preview={amnet.jpg}
}









